"""MCP æœåŠ¡å™¨è¯„ä¼°å·¥å…·

æ­¤è„šæœ¬é€šè¿‡ä½¿ç”¨ å¤§å‹è¯­è¨€æ¨¡å‹ å¯¹ MCP æœåŠ¡å™¨è¿è¡Œæµ‹è¯•é—®é¢˜æ¥è¯„ä¼°å®ƒä»¬ã€‚
"""

import argparse
import asyncio
import json
import re
import sys
import time
import traceback
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Any

from anthropic import Anthropic

from connections import create_connection

EVALUATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è®¿é—®å·¥å…·çš„ AI åŠ©æ‰‹ã€‚

å½“ç»™å‡ºä»»åŠ¡æ—¶ï¼Œä½ å¿…é¡»ï¼š
1. ä½¿ç”¨å¯ç”¨çš„å·¥å…·å®Œæˆä»»åŠ¡
2. åœ¨ <summary> æ ‡ç­¾ä¸­æä¾›æ¯ä¸€æ­¥æ–¹æ³•çš„æ€»ç»“
3. åœ¨ <feedback> æ ‡ç­¾ä¸­æä¾›å…³äºæ‰€æä¾›å·¥å…·çš„åé¦ˆ
4. åœ¨ <response> æ ‡ç­¾ä¸­æä¾›ä½ çš„æœ€ç»ˆå“åº”

æ€»ç»“è¦æ±‚ï¼š
- åœ¨ <summary> æ ‡ç­¾ä¸­ï¼Œä½ å¿…é¡»è§£é‡Šï¼š
  - ä½ ä¸ºå®Œæˆä»»åŠ¡æ‰€é‡‡å–çš„æ­¥éª¤
  - ä½ ä½¿ç”¨äº†å“ªäº›å·¥å…·ï¼ŒæŒ‰ä»€ä¹ˆé¡ºåºä½¿ç”¨ï¼Œä»¥åŠä¸ºä»€ä¹ˆ
  - ä½ æä¾›ç»™æ¯ä¸ªå·¥å…·çš„è¾“å…¥
  - ä½ ä»æ¯ä¸ªå·¥å…·æ”¶åˆ°çš„è¾“å‡º
  - å…³äºå¦‚ä½•å¾—å‡ºå“åº”çš„æ€»ç»“

åé¦ˆè¦æ±‚ï¼š
- åœ¨ <feedback> æ ‡ç­¾ä¸­ï¼Œæä¾›å…³äºå·¥å…·çš„å»ºè®¾æ€§åé¦ˆï¼š
  - è¯„è®ºå·¥å…·åç§°ï¼šå®ƒä»¬æ˜¯å¦æ¸…æ™°ä¸”å…·æœ‰æè¿°æ€§ï¼Ÿ
  - è¯„è®ºè¾“å…¥å‚æ•°ï¼šå®ƒä»¬æ˜¯å¦è®°å½•è‰¯å¥½ï¼Ÿå¿…éœ€å‚æ•°å’Œå¯é€‰å‚æ•°æ˜¯å¦æ¸…æ™°ï¼Ÿ
  - è¯„è®ºæè¿°ï¼šå®ƒä»¬æ˜¯å¦å‡†ç¡®æè¿°äº†å·¥å…·çš„åŠŸèƒ½ï¼Ÿ
  - è¯„è®ºåœ¨å·¥å…·ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°çš„ä»»ä½•é”™è¯¯ï¼šå·¥å…·æ˜¯å¦æ‰§è¡Œå¤±è´¥ï¼Ÿå·¥å…·æ˜¯å¦è¿”å›äº†å¤ªå¤š tokenï¼Ÿ
  - è¯†åˆ«å…·ä½“çš„æ”¹è¿›é¢†åŸŸå¹¶è§£é‡Šä¸ºä»€ä¹ˆå®ƒä»¬ä¼šæœ‰å¸®åŠ©
  - åœ¨ä½ çš„å»ºè®®ä¸­è¦å…·ä½“ä¸”å¯æ“ä½œ

å“åº”è¦æ±‚ï¼š
- ä½ çš„å“åº”åº”è¯¥ç®€æ´ï¼Œç›´æ¥å›ç­”æ‰€é—®çš„é—®é¢˜
- å§‹ç»ˆå°†ä½ çš„æœ€ç»ˆå“åº”åŒ…è£¹åœ¨ <response> æ ‡ç­¾ä¸­
- å¦‚æœæ— æ³•è§£å†³é—®é¢˜ï¼Œè¿”å› <response>NOT_FOUND</response>
- å¯¹äºæ•°å­—å“åº”ï¼Œåªæä¾›æ•°å­—
- å¯¹äº IDï¼Œåªæä¾› ID
- å¯¹äºåç§°æˆ–æ–‡æœ¬ï¼Œæä¾›è¯·æ±‚çš„ç¡®åˆ‡æ–‡æœ¬
- ä½ çš„å“åº”åº”è¯¥æ”¾åœ¨æœ€å"""


def parse_evaluation_file(file_path: Path) -> list[dict[str, Any]]:
    """è§£æåŒ…å« qa_pair å…ƒç´ çš„ XML è¯„ä¼°æ–‡ä»¶ã€‚"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        evaluations = []

        for qa_pair in root.findall(".//qa_pair"):
            question_elem = qa_pair.find("question")
            answer_elem = qa_pair.find("answer")

            if question_elem is not None and answer_elem is not None:
                evaluations.append({
                    "question": (question_elem.text or "").strip(),
                    "answer": (answer_elem.text or "").strip(),
                })

        return evaluations
    except Exception as e:
        print(f"è§£æè¯„ä¼°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return []


def extract_xml_content(text: str, tag: str) -> str | None:
    """ä» XML æ ‡ç­¾ä¸­æå–å†…å®¹ã€‚"""
    pattern = rf"<{tag}>(.*?)</{tag}>"
    matches = re.findall(pattern, text, re.DOTALL)
    return matches[-1].strip() if matches else None


async def agent_loop(
    client: Anthropic,
    model: str,
    question: str,
    tools: list[dict[str, Any]],
    connection: Any,
) -> tuple[str, dict[str, Any]]:
    """ä½¿ç”¨ MCP å·¥å…·è¿è¡Œæ™ºèƒ½ä½“å¾ªç¯ã€‚"""
    messages = [{"role": "user", "content": question}]

    response = await asyncio.to_thread(
        client.messages.create,
        model=model,
        max_tokens=4096,
        system=EVALUATION_PROMPT,
        messages=messages,
        tools=tools,
    )

    messages.append({"role": "assistant", "content": response.content})

    tool_metrics = {}

    while response.stop_reason == "tool_use":
        tool_use = next(block for block in response.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        tool_start_ts = time.time()
        try:
            tool_result = await connection.call_tool(tool_name, tool_input)
            tool_response = json.dumps(tool_result) if isinstance(tool_result, (dict, list)) else str(tool_result)
        except Exception as e:
            tool_response = f"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™: {str(e)}\n"
            tool_response += traceback.format_exc()
        tool_duration = time.time() - tool_start_ts

        if tool_name not in tool_metrics:
            tool_metrics[tool_name] = {"count": 0, "durations": []}
        tool_metrics[tool_name]["count"] += 1
        tool_metrics[tool_name]["durations"].append(tool_duration)

        messages.append({
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use.id,
                "content": tool_response,
            }]
        })

        response = await asyncio.to_thread(
            client.messages.create,
            model=model,
            max_tokens=4096,
            system=EVALUATION_PROMPT,
            messages=messages,
            tools=tools,
        )
        messages.append({"role": "assistant", "content": response.content})

    response_text = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )
    return response_text, tool_metrics


async def evaluate_single_task(
    client: Anthropic,
    model: str,
    qa_pair: dict[str, Any],
    tools: list[dict[str, Any]],
    connection: Any,
    task_index: int,
) -> dict[str, Any]:
    """ä½¿ç”¨ç»™å®šå·¥å…·è¯„ä¼°å•ä¸ªé—®ç­”å¯¹ã€‚"""
    start_time = time.time()

    print(f"ä»»åŠ¡ {task_index + 1}: æ­£åœ¨è¿è¡Œé—®é¢˜ä¸º: {qa_pair['question']} çš„ä»»åŠ¡")
    response, tool_metrics = await agent_loop(client, model, qa_pair["question"], tools, connection)

    response_value = extract_xml_content(response, "response")
    summary = extract_xml_content(response, "summary")
    feedback = extract_xml_content(response, "feedback")

    duration_seconds = time.time() - start_time

    return {
        "question": qa_pair["question"],
        "expected": qa_pair["answer"],
        "actual": response_value,
        "score": int(response_value == qa_pair["answer"]) if response_value else 0,
        "total_duration": duration_seconds,
        "tool_calls": tool_metrics,
        "num_tool_calls": sum(len(metrics["durations"]) for metrics in tool_metrics.values()),
        "summary": summary,
        "feedback": feedback,
    }


REPORT_HEADER = """
# è¯„ä¼°æŠ¥å‘Š

## æ‘˜è¦

- **å‡†ç¡®ç‡**: {correct}/{total} ({accuracy:.1f}%)
- **å¹³å‡ä»»åŠ¡æŒç»­æ—¶é—´**: {average_duration_s:.2f}ç§’
- **æ¯ä¸ªä»»åŠ¡çš„å¹³å‡å·¥å…·è°ƒç”¨æ¬¡æ•°**: {average_tool_calls:.2f}
- **æ€»å·¥å…·è°ƒç”¨æ¬¡æ•°**: {total_tool_calls}

---
"""

TASK_TEMPLATE = """
### ä»»åŠ¡ {task_num}

**é—®é¢˜**: {question}
**æ ‡å‡†ç­”æ¡ˆ**: `{expected_answer}`
**å®é™…ç­”æ¡ˆ**: `{actual_answer}`
**æ­£ç¡®**: {correct_indicator}
**æŒç»­æ—¶é—´**: {total_duration:.2f}ç§’
**å·¥å…·è°ƒç”¨**: {tool_calls}

**æ€»ç»“**
{summary}

**åé¦ˆ**
{feedback}

---
"""


async def run_evaluation(
    eval_path: Path,
    connection: Any,
    model: str = "å¤§å‹è¯­è¨€æ¨¡å‹-3-7-sonnet-20250219",
) -> str:
    """ä½¿ç”¨ MCP æœåŠ¡å™¨å·¥å…·è¿è¡Œè¯„ä¼°ã€‚"""
    print("ğŸš€ æ­£åœ¨å¼€å§‹è¯„ä¼°")

    client = Anthropic()

    tools = await connection.list_tools()
    print(f"ğŸ“‹ ä» MCP æœåŠ¡å™¨åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")

    qa_pairs = parse_evaluation_file(eval_path)
    print(f"ğŸ“‹ åŠ è½½äº† {len(qa_pairs)} ä¸ªè¯„ä¼°ä»»åŠ¡")

    results = []
    for i, qa_pair in enumerate(qa_pairs):
        print(f"æ­£åœ¨å¤„ç†ä»»åŠ¡ {i + 1}/{len(qa_pairs)}")
        result = await evaluate_single_task(client, model, qa_pair, tools, connection, i)
        results.append(result)

    correct = sum(r["score"] for r in results)
    accuracy = (correct / len(results)) * 100 if results else 0
    average_duration_s = sum(r["total_duration"] for r in results) / len(results) if results else 0
    average_tool_calls = sum(r["num_tool_calls"] for r in results) / len(results) if results else 0
    total_tool_calls = sum(r["num_tool_calls"] for r in results)

    report = REPORT_HEADER.format(
        correct=correct,
        total=len(results),
        accuracy=accuracy,
        average_duration_s=average_duration_s,
        average_tool_calls=average_tool_calls,
        total_tool_calls=total_tool_calls,
    )

    report += "".join([
        TASK_TEMPLATE.format(
            task_num=i + 1,
            question=qa_pair["question"],
            expected_answer=qa_pair["answer"],
            actual_answer=result["actual"] or "N/A",
            correct_indicator="âœ…" if result["score"] else "âŒ",
            total_duration=result["total_duration"],
            tool_calls=json.dumps(result["tool_calls"], indent=2),
            summary=result["summary"] or "N/A",
            feedback=result["feedback"] or "N/A",
        )
        for i, (qa_pair, result) in enumerate(zip(qa_pairs, results))
    ])

    return report


def parse_headers(header_list: list[str]) -> dict[str, str]:
    """å°† 'Key: Value' æ ¼å¼çš„æ ‡å¤´å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸ã€‚"""
    headers = {}
    if not header_list:
        return headers

    for header in header_list:
        if ":" in header:
            key, value = header.split(":", 1)
            headers[key.strip()] = value.strip()
        else:
            print(f"è­¦å‘Š: å¿½ç•¥æ ¼å¼é”™è¯¯çš„æ ‡å¤´: {header}")
    return headers


def parse_env_vars(env_list: list[str]) -> dict[str, str]:
    """å°† 'KEY=VALUE' æ ¼å¼çš„ç¯å¢ƒå˜é‡å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸ã€‚"""
    env = {}
    if not env_list:
        return env

    for env_var in env_list:
        if "=" in env_var:
            key, value = env_var.split("=", 1)
            env[key.strip()] = value.strip()
        else:
            print(f"è­¦å‘Š: å¿½ç•¥æ ¼å¼é”™è¯¯çš„ç¯å¢ƒå˜é‡: {env_var}")
    return env


async def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨æµ‹è¯•é—®é¢˜è¯„ä¼° MCP æœåŠ¡å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¯„ä¼°æœ¬åœ° stdio MCP æœåŠ¡å™¨
  python evaluation.py -t stdio -c python -a my_server.py eval.xml

  # è¯„ä¼° SSE MCP æœåŠ¡å™¨
  python evaluation.py -t sse -u https://example.com/mcp -H "Authorization: Bearer token" eval.xml

  # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è¯„ä¼° HTTP MCP æœåŠ¡å™¨
  python evaluation.py -t http -u https://example.com/mcp -m å¤§å‹è¯­è¨€æ¨¡å‹-3-5-sonnet-20241022 eval.xml
        """,
    )

    parser.add_argument("eval_file", type=Path, help="è¯„ä¼° XML æ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("-t", "--transport", choices=["stdio", "sse", "http"], default="stdio", help="ä¼ è¾“ç±»å‹ï¼ˆé»˜è®¤å€¼: stdioï¼‰")
    parser.add_argument("-m", "--model", default="å¤§å‹è¯­è¨€æ¨¡å‹-3-7-sonnet-20250219", help="è¦ä½¿ç”¨çš„ å¤§å‹è¯­è¨€æ¨¡å‹ æ¨¡å‹ï¼ˆé»˜è®¤å€¼: å¤§å‹è¯­è¨€æ¨¡å‹-3-7-sonnet-20250219ï¼‰")

    stdio_group = parser.add_argument_group("stdio é€‰é¡¹")
    stdio_group.add_argument("-c", "--command", help="è¿è¡Œ MCP æœåŠ¡å™¨çš„å‘½ä»¤ï¼ˆä»… stdioï¼‰")
    stdio_group.add_argument("-a", "--args", nargs="+", help="å‘½ä»¤çš„å‚æ•°ï¼ˆä»… stdioï¼‰")
    stdio_group.add_argument("-e", "--env", nargs="+", help="KEY=VALUE æ ¼å¼çš„ç¯å¢ƒå˜é‡ï¼ˆä»… stdioï¼‰")

    remote_group = parser.add_argument_group("sse/http é€‰é¡¹")
    remote_group.add_argument("-u", "--url", help="MCP æœåŠ¡å™¨ URLï¼ˆsse/http ä»…ï¼‰")
    remote_group.add_argument("-H", "--header", nargs="+", dest="headers", help="'Key: Value' æ ¼å¼çš„ HTTP æ ‡å¤´ï¼ˆsse/http ä»…ï¼‰")

    parser.add_argument("-o", "--output", type=Path, help="è¯„ä¼°æŠ¥å‘Šçš„è¾“å‡ºæ–‡ä»¶ï¼ˆé»˜è®¤å€¼: stdoutï¼‰")

    args = parser.parse_args()

    if not args.eval_file.exists():
        print(f"é”™è¯¯: æœªæ‰¾åˆ°è¯„ä¼°æ–‡ä»¶: {args.eval_file}")
        sys.exit(1)

    headers = parse_headers(args.headers) if args.headers else None
    env_vars = parse_env_vars(args.env) if args.env else None

    try:
        connection = create_connection(
            transport=args.transport,
            command=args.command,
            args=args.args,
            env=env_vars,
            url=args.url,
            headers=headers,
        )
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

    print(f"ğŸ”— æ­£åœ¨é€šè¿‡ {args.transport} è¿æ¥åˆ° MCP æœåŠ¡å™¨...")

    async with connection:
        print("âœ… è¿æ¥æˆåŠŸ")
        report = await run_evaluation(args.eval_file, connection, args.model)

        if args.output:
            args.output.write_text(report)
            print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {args.output}")
        else:
            print("\n" + report)


if __name__ == "__main__":
    asyncio.run(main())
